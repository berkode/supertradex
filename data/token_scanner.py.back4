import os
import sys
import time
import random
import logging
from typing import List, Set, Dict, Any, Optional, Union, TypedDict
import requests
import pandas as pd
from random import randint
from dotenv import load_dotenv
from datetime import datetime, timezone
from utils import get_logger
from filters.twitter_check import TwitterCheck
from utils.exception_handler import ExceptionHandler
from config.settings import Settings
from config.thresholds import Thresholds
from config.filters_config import FiltersConfig
from config.dexscreener_api import DexScreenerAPI
from filters.whitelist import Whitelist
from data.filtering import Filtering
import csv
import asyncio
import importlib
import json
import httpx
from data.token_database import TokenDatabase
from data.platform_tracker import PlatformTracker
import aiohttp
from utils.proxy_manager import ProxyManager
import numpy as np

# # Dynamically construct the path to the secrets.env file
# script_dir = os.path.dirname(os.path.abspath(__file__))
# env_path = os.path.join(script_dir, '.env')
# Load environment variables from .env
load_dotenv("config/.env")

# Constants
mint = ""
chainId = os.getenv('CHAIN_ID')
tokens = []
max_retries = 2
retry_delay = randint(5, 16) #seconds

# Links and keys from secrets
SOLSNIFFER_API_KEY = os.getenv('SOLSNIFFER_API_KEY')
SOLSNIFFER_API_URL = os.getenv('SOLSNIFFER_API_URL')  # Solsniffer API endpoint
MIN_SOLSNIFFER_SCORE = int(os.getenv('MIN_SOLSNIFFER_SCORE'))
DEXSCREENER_TOKEN_QTY = int(os.getenv('DEXSCREENER_TOKEN_QTY'))
DEXSCREENER_TOKEN_TOP_QTY = int(os.getenv('DEXSCREENER_TOKEN_TOP_QTY'))
# TWEETSCOUT_API_KEY = os.getenv('TWEETSCOUT_API_KEY')
TWEETSCOUT_URL = os.getenv('TWEETSCOUT_URL')  # Tweetscout API https://app.tweetscout.io/search?q=https://x.com/pumpkindotfun
SOLANATRACKER_API_KEY = os.getenv('SOLANATRACKER_API_KEY')
SOLANATRACKER_URL = os.getenv('SOLANATRACKER_URL')
DEXSCREENER_API_BASE_URL = os.getenv('DEXSCREENER_API_BASE_URL')
DEXSCREENER_LATEST = os.getenv('DEXSCREENER_LATEST')
DEXSCREENER_DETAILS = os.getenv('DEXSCREENER_DETAILS')
RUGCHECK_URL = os.getenv('RUGCHECK_URL')

ALCHEMY_API_KEY = os.getenv('ALCHEMY_API_KEY')
# HELIUS API key
HELIUS_API_KEY = os.getenv('HELIUS_API_KEY')
BUY_LIMIT_AMOUNT = float(os.getenv('BUY_LIMIT_AMOUNT', '0.01'))  # Amount in SOLANA to buy

# # Initialize HELIUS client
# helius_client = HeliusClient(api_key=HELIUS_API_KEY)

# # Initialize SynthronTrader
# trader = SynthronTrader()

# Get paths from environment
OUTPUT_DIR = os.getenv('OUTPUT_DIR', 'outputs')
WHITELIST_PATH = os.getenv('WHITELIST_PATH', f'{OUTPUT_DIR}/whitelist.csv')
FILTERS_DIRECTORY = os.getenv('FILTERS_DIRECTORY', 'synthron/filters')

# Ensure output directory exists
os.makedirs(os.path.dirname(WHITELIST_PATH), exist_ok=True)

# Type hints for proxy manager
class ProxyDict(TypedDict):
    http: str
    https: str

class TokenScanner:
    def __init__(self, db: TokenDatabase, http_client: httpx.AsyncClient, settings: Settings, thresholds: Thresholds):
        self.db = db
        self.http_client = http_client
        self.settings = settings
        self.thresholds = thresholds
        self.logger = get_logger("TokenScanner")
        self.logger.setLevel(logging.INFO)
        # Initialize proxy manager with type hints
        self.proxy_manager: ProxyManager = ProxyManager()
        # Initialize other attributes
        self.filters_config = FiltersConfig()
        self.filtering = Filtering()  # Initialize the centralized filtering
        
        # Initialize scan_results dictionary
        self.scan_results = {
            'last_scan_time': None,
            'tokens': pd.DataFrame()
        }
        self.dexscreener_api = DexScreenerAPI()
        self.platform_tracker = PlatformTracker(db, settings, thresholds)
        # asyncio.run(self.scan_tokens())  # Run the main function in the initializer

    # Function to format volume and USD values
    def format_value(self, value):
        if value >= 1_000_000_000:
            return f"{value / 1_000_000_000:.2f}B"
        elif value >= 1_000_000:
            return f"{value / 1_000_000:.2f}M"
        elif value >= 1_000:
            return f"{value / 1_000:.2f}K"
        else:
            return f"{value:.2f}"

    # Function to convert formatted values back to numeric
    def convert_to_numeric(self, value):
        if 'B' in value:
            return float(value[:-1]) * 1_000_000_000
        elif 'M' in value:
            return float(value[:-1]) * 1_000_000
        elif 'K' in value:
            return float(value[:-1]) * 1_000
        else:
            return float(value)

    def dexscreener(self):
        """Fetch trending tokens from DexScreener API."""
        return self.dexscreener_api.get_trending_tokens_sync()

    def solanatracker(self):
        """Fetch trending tokens from Solana Tracker and sort them in ascending order of highest Volume/MarketCap."""
        trending_url = f"{SOLANATRACKER_URL}/tokens/trending"
        token_url = f"{SOLANATRACKER_URL}/tokens/{mint}"
        headers = {
            "x-api-key": SOLANATRACKER_API_KEY
        }
        
        print(f"SolanaTracker fetching token data from: {token_url}\n")
        
        for attempt in range(max_retries):
            try:
                response = requests.get(token_url, headers=headers)
                response.raise_for_status()
                trending_data = response.json()

                # Debug: Print the structure of trending_data
                print(f"SolanaTracker trending Data:\n{trending_data}\n")

                # Ensure trending_data is a list of dictionaries
                if isinstance(trending_data, list) and all(isinstance(token, dict) for token in trending_data):
                    # Filter tokens based on criteria
                    filtered_tokens = []
                    for token in trending_data:
                        token_data = token['token']
                        pools_data = token['pools'][0]
                        txns_data = pools_data['txns']
                        events_data = token['events']
                        risk_data = token['risk']
                        buysCount_data = token['buysCount']
                        sellsCount_data = token['sellsCount']

                        mint = token_data['mint']
                        symbol = token_data['symbol']
                        twitter = token_data.get('twitter', '')
                        creator_site = token_data.get('website', '')
                        has_file_metadata = token_data['hasFileMetaData']

                        price = pools_data['price']['usd']
                        market_cap_usd = pools_data['marketCap']['usd']
                        liquidity_usd = pools_data['liquidity']['usd']
                        txns_volume = txns_data['volume']
                        buysCount = buysCount_data
                        txns_buys = txns_data['buys']
                        sellsCount = sellsCount_data
                        txns_sells = txns_data['sells']
                        lpburn = pools_data['lpBurn']

                        price_change_1m = round(events_data.get('1m', {}).get('priceChangePercentage', 0) or 0, 2)
                        price_change_5m = round(events_data.get('5m', {}).get('priceChangePercentage', 0) or 0, 2)
                        price_change_1h = round(events_data.get('1h', {}).get('priceChangePercentage', 0) or 0, 2)
                        price_change_24h = round(events_data.get('24h', {}).get('priceChangePercentage', 0) or 0, 2)

                        rugged = risk_data['rugged']
                        risk_score = risk_data['score']
                        total_risk_score = sum(risk['score'] for risk in risk_data['risks'])
                        jupiter_verified = risk_data.get('jupiterVerified', False)
                        
                        # Apply filters
                        # if not twitter:
                        #     continue
                        # if not has_file_metadata:
                        #     continue
                        # if market_cap_usd < 7500:
                        #     continue
                        # if liquidity_usd < 7500:
                        #     continue
                        # if txns_volume < 5000:
                        #     continue
                        if lpburn < 90:
                            continue
                        if rugged:
                            continue
                        if risk_score > 5:
                            continue

                        filtered_tokens.append(token)

                    # Sort tokens by Volume/MarketCap in ascending order
                    sorted_tokens = sorted(filtered_tokens, key=lambda x: x['pools'][0]['liquidity']['quote'] / x['pools'][0]['marketCap']['quote'] if x['pools'] else float('inf'))
                    return sorted_tokens
                else:
                    self.logger.warning("SolanaTracker unexpected data format received.")
                    return []
                
            except Exception as e:
                self.logger.error(f"SolanaTracker error fetching data: {e}")
                if attempt < max_retries - 1:
                    self.logger.error(f"Retrying in {retry_delay} seconds...")
                    time.sleep(retry_delay)
                else:
                    return []

    def rugcheck(self, df):
        """Apply RugCheck filter to tokens DataFrame using rugcheck.xyz API."""
        if df.empty:
            return pd.DataFrame()
        
        # Initialize rugcheck_score column
        df['rugcheckScore'] = 100
        initial_count = len(df)
        
        for index, row in df.iterrows():
            mint = row['mint']
            max_retries = 2  # Reduced from 5 to 2 retries
            base_delay = 3  # Base delay in seconds
            max_delay = 30  # Maximum delay in seconds
            
            for attempt in range(max_retries):
                try:
                    # Add exponential backoff delay between requests
                    if attempt > 0:
                        wait_time = min(
                            base_delay * (2 ** attempt) + random.uniform(0, 2),
                            max_delay
                        )
                        self.logger.info(f"Waiting {wait_time:.1f} seconds before retry {attempt + 1} for {mint}")
                        time.sleep(wait_time)
                    else:
                        # Initial delay between requests
                        time.sleep(2)
                    
                    # Get proxy for this request
                    proxy = self.proxy_manager.get_proxy_dict()
                    
                    # Call rugcheck.xyz API
                    rugcheck_url = f"{RUGCHECK_URL}/tokens/{mint}/report/summary"
                    response = requests.get(
                        rugcheck_url,
                        proxies=proxy,
                        timeout=30
                    )
                    
                    # Handle different response status codes
                    if response.status_code == 429:  # Too Many Requests
                        if proxy:
                            self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                        if attempt < max_retries - 1:
                            self.logger.warning(
                                f"Rate limit hit for {mint}, "
                                f"attempt {attempt + 1}/{max_retries}"
                            )
                            continue
                        else:
                            self.logger.error(f"Max retries reached for {mint} due to rate limits")
                            break
                        
                    elif response.status_code == 504:  # Gateway Timeout
                        if proxy:
                            self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                        if attempt < max_retries - 1:
                            self.logger.warning(
                                f"Gateway timeout for {mint}, "
                                f"attempt {attempt + 1}/{max_retries}"
                            )
                            continue
                        else:
                            self.logger.error(f"Max retries reached for {mint} due to timeouts")
                            break
                    
                    response.raise_for_status()
                    data = response.json()
                    
                    # Extract rugcheck score from API response
                    rugcheck_score = int(data.get('score_normalised', 100))
                    df.at[index, 'rugcheckScore'] = rugcheck_score
                    self.logger.info(f"Rugcheck score {rugcheck_score} for {mint}")
                    break  # Success, exit retry loop
                    
                except requests.exceptions.Timeout:
                    if proxy:
                        self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Timeout on attempt {attempt + 1} for {mint}")
                        continue
                    else:
                        self.logger.error(f"Max retries reached due to timeouts for {mint}")
                        break
                    
                except requests.exceptions.RequestException as e:
                    if proxy:
                        self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Request error on attempt {attempt + 1} for {mint}: {e}")
                        continue
                    else:
                        self.logger.error(f"Max retries reached due to request errors for {mint}: {e}")
                        break
                
                except Exception as e:
                    self.logger.error(f"Unexpected error processing {mint}: {e}", exc_info=True)
                    break
        
        # Filter based on minimum score threshold from settings
        max_score = float(os.getenv('MAX_RUGCHECK_SCORE', '100'))
        df = df[df['rugcheckScore'] < max_score]
        
        self.logger.info(f"Rugcheck filter: {initial_count} tokens → {len(df)} tokens passed.")
        return df

    def solsniffer(self, df):
        """Fetches the Solsniffer score and additional factors for tokens with improved rate limiting."""
        if df.empty:
            return pd.DataFrame()

        filtered_data = []
        max_retries = 2  # Reduced from 3 to 2 retries
        base_delay = 5  # Base delay in seconds
        max_delay = 30  # Maximum delay in seconds
        
        for index, row in df.iterrows():
            mint = row['mint']
            for attempt in range(max_retries):
                try:
                    # Add exponential backoff delay between requests
                    if attempt > 0:
                        wait_time = min(
                            base_delay * (2 ** attempt) + random.uniform(0, 2),
                            max_delay
                        )
                        self.logger.info(f"Waiting {wait_time:.1f} seconds before retry {attempt + 1} for {mint}")
                        time.sleep(wait_time)
                    else:
                        # Initial delay between requests
                        time.sleep(2)
                    
                    # Get proxy for this request
                    proxy = self.proxy_manager.get_proxy_dict()
                    
                    solsniffer_url = f"{SOLSNIFFER_API_URL}/token/refresh/{mint}"
                    response = requests.get(
                        solsniffer_url, 
                        headers={"X-API-KEY": SOLSNIFFER_API_KEY},
                        proxies=proxy,
                        timeout=30
                    )
                    
                    # Handle different response status codes
                    if response.status_code == 429:  # Too Many Requests
                        if proxy:
                            self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                        if attempt < max_retries - 1:
                            self.logger.warning(
                                f"Rate limit hit for {mint}, "
                                f"attempt {attempt + 1}/{max_retries}"
                            )
                            continue
                        else:
                            self.logger.warning(f"Max retries reached for {mint} due to rate limits, assigning minimum score")
                            # Assign minimum score and continue
                            row_dict = row.to_dict()
                            row_dict.update({
                                'solsnifferScore': MIN_SOLSNIFFER_SCORE,
                                'mintDisabled': True,
                                'freezeDisabled': True,
                                'lpBurned': True,
                                'top10Holders': False,
                                'top10Holders%': 0,
                                'topHolder%': 0,
                                'totalHolders': 0,
                                'totalHolders%': 0,
                                'highRiskCount': 0,
                                'moderateRiskCount': 0,
                                'lowRiskCount': 0,
                                'specificRiskCount': 0
                            })
                            filtered_data.append(row_dict)
                            self.logger.info(f"Assigned minimum score {MIN_SOLSNIFFER_SCORE} to {mint} due to API limits")
                            break
                    
                    response.raise_for_status()
                    data = response.json()
                    
                    # Check if the response indicates any errors
                    if 'error' in data:
                        self.logger.warning(f"SolSniffer API returned error for {mint}: {data['error']}")
                        if attempt < max_retries - 1:
                            continue
                        else:
                            # Assign minimum score and continue
                            row_dict = row.to_dict()
                            row_dict.update({
                                'solsnifferScore': MIN_SOLSNIFFER_SCORE,
                                'mintDisabled': True,
                                'freezeDisabled': True,
                                'lpBurned': True,
                                'top10Holders': False,
                                'top10Holders%': 0,
                                'topHolder%': 0,
                                'totalHolders': 0,
                                'totalHolders%': 0,
                                'highRiskCount': 0,
                                'moderateRiskCount': 0,
                                'lowRiskCount': 0,
                                'specificRiskCount': 0
                            })
                            filtered_data.append(row_dict)
                            self.logger.info(f"Assigned minimum score {MIN_SOLSNIFFER_SCORE} to {mint} due to API error")
                            break
                    
                    # Get the overall score and additional factors
                    solsniffer_score = int(contract_data.get("score", 0))
                    audit_risk = contract_data.get("auditRisk", {})
                    mint_disabled = bool(audit_risk.get("mintDisabled", False))
                    freeze_disabled = bool(audit_risk.get("freezeDisabled", False))
                    lp_burned = bool(audit_risk.get("lpBurned", False))
                    top10_holders = bool(audit_risk.get("top10Holders", False))
                    
                    if solsniffer_score >= MIN_SOLSNIFFER_SCORE and mint_disabled and freeze_disabled:
                        row_dict = row.to_dict()
                        row_dict.update({
                            'solsnifferScore': solsniffer_score,
                            'mintDisabled': mint_disabled,
                            'freezeDisabled': freeze_disabled,
                            'lpBurned': lp_burned,
                            'top10Holders': top10_holders,
                            'top10Holders%': round(top10_percentage, 2),
                            'topHolder%': round(top_holder_percentage, 2),
                            'totalHolders': total_holders,
                            'totalHolders%': round(total_percentage, 2),
                            'highRiskCount': high_count,
                            'moderateRiskCount': moderate_count,
                            'lowRiskCount': low_count,
                            'specificRiskCount': specific_count
                        })
                        filtered_data.append(row_dict)
                        self.logger.info(f"Solsniffer passed score {solsniffer_score}>{MIN_SOLSNIFFER_SCORE} and mint/freeze disabled for {mint}.")
                        break  # Success, exit retry loop
                    else:
                        self.logger.warning(f"Solsniffer skipping score {solsniffer_score}<{MIN_SOLSNIFFER_SCORE} or mint/freeze not-disabled for {mint}.")
                        break  # Not passing criteria, no need to retry
                
                except requests.exceptions.Timeout:
                    if proxy:
                        self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Timeout on attempt {attempt + 1} for {mint}")
                        continue
                    else:
                        self.logger.warning(f"Max retries reached due to timeouts for {mint}, assigning minimum score")
                        # Assign minimum score and continue
                        row_dict = row.to_dict()
                        row_dict.update({
                            'solsnifferScore': MIN_SOLSNIFFER_SCORE,
                            'mintDisabled': True,
                            'freezeDisabled': True,
                            'lpBurned': True,
                            'top10Holders': False,
                            'top10Holders%': 0,
                            'topHolder%': 0,
                            'totalHolders': 0,
                            'totalHolders%': 0,
                            'highRiskCount': 0,
                            'moderateRiskCount': 0,
                            'lowRiskCount': 0,
                            'specificRiskCount': 0
                        })
                        filtered_data.append(row_dict)
                        self.logger.info(f"Assigned minimum score {MIN_SOLSNIFFER_SCORE} to {mint} due to timeouts")
                        break
                
                except requests.exceptions.RequestException as e:
                    if proxy:
                        self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                    if attempt < max_retries - 1:
                        self.logger.warning(f"Request error on attempt {attempt + 1} for {mint}: {e}")
                        continue
                    else:
                        self.logger.warning(f"Max retries reached due to request errors for {mint}, assigning minimum score")
                        # Assign minimum score and continue
                        row_dict = row.to_dict()
                        row_dict.update({
                            'solsnifferScore': MIN_SOLSNIFFER_SCORE,
                            'mintDisabled': True,
                            'freezeDisabled': True,
                            'lpBurned': True,
                            'top10Holders': False,
                            'top10Holders%': 0,
                            'topHolder%': 0,
                            'totalHolders': 0,
                            'totalHolders%': 0,
                            'highRiskCount': 0,
                            'moderateRiskCount': 0,
                            'lowRiskCount': 0,
                            'specificRiskCount': 0
                        })
                        filtered_data.append(row_dict)
                        self.logger.info(f"Assigned minimum score {MIN_SOLSNIFFER_SCORE} to {mint} due to request error")
                        break
                
                except Exception as e:
                    self.logger.error(f"Unexpected error processing {mint}: {e}", exc_info=True)
                    # Assign minimum score and continue
                    row_dict = row.to_dict()
                    row_dict.update({
                        'solsnifferScore': MIN_SOLSNIFFER_SCORE,
                        'mintDisabled': True,
                        'freezeDisabled': True,
                        'lpBurned': True,
                        'top10Holders': False,
                        'top10Holders%': 0,
                        'topHolder%': 0,
                        'totalHolders': 0,
                        'totalHolders%': 0,
                        'highRiskCount': 0,
                        'moderateRiskCount': 0,
                        'lowRiskCount': 0,
                        'specificRiskCount': 0
                    })
                    filtered_data.append(row_dict)
                    self.logger.info(f"Assigned minimum score {MIN_SOLSNIFFER_SCORE} to {mint} due to unexpected error")
                    break
        
        self.logger.info(f"Solsniffer filter: {len(df)} tokens → {len(filtered_data)} tokens passed:\n{filtered_data}\n")
        return pd.DataFrame(filtered_data) if filtered_data else pd.DataFrame()

    # async def tweetscout(self, twitter):
    #     """Fetches TweetScout data for a Twitter handle using Playwright."""
    #     for attempt in range(max_retries):
    #         try:
    #             async with async_playwright() as p:
    #                 browser = await p.chromium.launch(headless=False)  # Run in headful mode to better handle Cloudflare
    #                 context = await browser.new_context()
    #                 page = await context.new_page()
    #                 await page.goto(f"{TWEETSCOUT_URL}{twitter}")

    #                 # Wait for Cloudflare challenge to complete
    #                 await page.wait_for_load_state('networkidle')

    #                 # Check for CAPTCHA and prompt user to solve it
    #                 if await page.query_selector("iframe[src*='captcha']"):
    #                     print("CAPTCHA detected. Please solve it manually.")
    #                     await page.wait_for_selector("p.Counter_text_big__Xkphn", timeout=120000)  # Wait for user to solve CAPTCHA

    #                 # Increase the timeout for waiting for the selector
    #                 await page.wait_for_selector("p.Counter_text_big__Xkphn", timeout=60000)  # Increased timeout to 60 seconds

    #                 # Extract overall score using BeautifulSoup
    #                 html_content = await page.content()
    #                 soup_obj = soup(html_content, "html.parser")
    #                 try:
    #                     score_element = soup_obj.find("p", class_="Counter_text_big__Xkphn")
    #                     if score_element:
    #                         score = int(score_element.text.strip())
    #                     else:
    #                         print("Could not find overall score on Tweetscout.")
    #                         return None, None
    #                 except (AttributeError, ValueError):
    #                     print("Error extracting overall score.")
    #                     return None, None

    #                 # Extract total followers by summing up the three numbers
    #                 try:
    #                     follower_elements = soup_obj.find_all("p", class_="ValueDelta_text__RZa6u")
    #                     followers = sum(int(element.text.strip()) for element in follower_elements)
    #                 except (AttributeError, ValueError):
    #                     print("Error extracting total followers.")
    #                     return None, None

    #                 await browser.close()  # Close the browser

    #                 return score, followers

    #         except Exception as e:
    #             print(f"Error fetching data from TweetScout: {e}")
    #             if attempt < max_retries - 1:
    #                 print(f"Retrying in {retry_delay} seconds...")
    #                 await asyncio.sleep(retry_delay)
    #             else:
    #                 return None, None

    # async def trade_tokens(self, df_final_tokens):
        # for index, row in df_final_tokens.iterrows():
        #     mint = row['mint']
        #     twitter = row['twitter']
        #     status = row['status']
            
        #     # Fetch trading data using HELIUS API
        #     trading_data = await helius_client.get_trading_data(mint)
        #     buy_txns = trading_data['buy_txns']
        #     sell_txns = trading_data['sell_txns']
        #     net_volume = trading_data['net_volume']
        #     rsi = trading_data['rsi']
            
        #     # Buy conditions
        #     if buy_txns > sell_txns and net_volume > 0 and rsi < 50:
        #         print(f"Buying {mint} with {BUY_LIMIT_AMOUNT} SOLANA")
        #         await trader.buy(mint, BUY_LIMIT_AMOUNT)
            
        #     # Sell conditions
        #     current_price = trading_data['current_price']
        #     buy_price = row['buy_price']
        #     price_change = (current_price - buy_price) / buy_price * 100
            
        #     if price_change <= -20:
        #         print(f"Stop loss triggered for {mint}, selling 100%")
        #         await trader.sell(mint, 100)
        #     elif price_change >= 20 and price_change < 60:
        #         print(f"Take profit 20% triggered for {mint}, selling 50%")
        #         await trader.sell(mint, 50)
        #     elif price_change >= 60:
        #         print(f"Take profit 60% triggered for {mint}, selling remaining amount")
        #         await trader.sell(mint, 100)
        #     elif trading_data['dev_sell'] >= 15:
        #         print(f"Developer sell 15% triggered for {mint}, selling 100%")
        #         await trader.sell(mint, 100)

    @ExceptionHandler.validate_and_handle(exception_type=Exception, context="TokenScanner.scan_tokens")
    async def scan_tokens(self):
        """
        Main scanning function that applies filters in sequence:
        1. Get tokens from DexScreener
        2. Apply RugCheck filter
        3. Apply SolSniffer filter
        4. Apply Twitter verification
        5. Update whitelist with qualified tokens
        """
        start_time = time.time()
        self.logger.info("Starting token scan")

        # Step 1: Get trending tokens from DexScreener
        df_dexscreener_tokens = await self.dexscreener_api.get_trending_tokens()
        if df_dexscreener_tokens.empty:
            self.logger.info("No trending tokens found on DexScreener")
            return pd.DataFrame()

        total_tokens = len(df_dexscreener_tokens)
        self.logger.info(f"Found {total_tokens} trending tokens on DexScreener")
        self.logger.info(f"DexScreener Tokens DataFrame:\n{df_dexscreener_tokens}")

        # Step 2: Apply RugCheck filter
        df_rugchecked_tokens = self.rugcheck(df_dexscreener_tokens)
        if df_rugchecked_tokens.empty:
            self.logger.info("No tokens passed RugCheck")
            return pd.DataFrame()
        self.logger.info(f"RugCheck Tokens DataFrame:\n{df_rugchecked_tokens}")

        # Step 3: Apply SolSniffer filter
        df_solsniffer_tokens = self.solsniffer(df_rugchecked_tokens)
        if df_solsniffer_tokens.empty:
            self.logger.info("No tokens passed SolSniffer checks")
            return pd.DataFrame()
        self.logger.info(f"SolSniffer Tokens DataFrame:\n{df_solsniffer_tokens}")

        # Add status column with default value 'pending'
        df_solsniffer_tokens['status'] = 'pending'

        # Step 4: Apply Twitter verification
        twitter_checker = TwitterCheck()
        df_final_tokens = await twitter_checker.filter_tokens_by_twitter(df_solsniffer_tokens)
        if df_final_tokens.empty:
            self.logger.info("No tokens passed Twitter verification")
            return pd.DataFrame()
        self.logger.info(f"Final Tokens DataFrame:\n{df_final_tokens}")

        # Step 5: Track platform status for each token
        for token in df_final_tokens.to_dict('records'):
            try:
                platform_status = await self.platform_tracker.track_platform_status(token)
                if platform_status:
                    token["platform_status"] = platform_status
            except Exception as e:
                self.logger.error(f"Error tracking platform status for {token['mint']}: {e}")

        # Step 6: Apply additional RugPullFilter checks
        filtered_tokens = await self.apply_rug_pull_filter(df_final_tokens.to_dict('records'))
        
        # Step 7: Apply other filters from filters directory
        final_tokens = await self.apply_additional_filters(filtered_tokens)
        
        # Step 8: Save to whitelist.csv directly
        try:
            os.makedirs(os.path.dirname(WHITELIST_PATH), exist_ok=True)
            df_final_tokens.to_csv(WHITELIST_PATH, index=False)
            self.logger.info(f"Saved {len(df_final_tokens)} tokens to whitelist.csv")
            
            # Convert DataFrame to list of dictionaries for database storage
            for _, row in df_final_tokens.iterrows():
                token_data = {
                    'mint': row['mint'],  # Use mint instead of address
                    'symbol': row['symbol'],
                    'name': row['name'],
                    'market_cap': row['market_cap'],
                    'liquidity': row['liquidity'],
                    'price': row['price']
                }
                
                # Add to database
                await self.db.add_or_update_token(token_data)
                
                # Add to whitelist
                await self.db.add_to_whitelist(token_data['mint'])
                
                self.logger.info(f"Added token {token_data['mint']} to database with whitelisted status")
        except Exception as e:
            self.logger.error(f"Error saving to whitelist.csv or database: {e}")

        # Update scan results
        self.scan_results['last_scan_time'] = datetime.now()
        self.scan_results['tokens'] = df_final_tokens

        execution_time = time.time() - start_time
        self.logger.info(f"Scan completed in {execution_time:.2f}s. Found {len(df_final_tokens)} qualified tokens.")

        return df_final_tokens

    def scan_tokens_sync(self):
        """Synchronous wrapper for scan_tokens"""
        try:
            return asyncio.run(self.scan_tokens())
        except Exception as e:
            self.logger.error(f"Error in scan_tokens_sync: {e}")
            return pd.DataFrame()

    def save_to_whitelist(self, filtered_tokens: List[dict]):
        """
        Saves complete token scanner results to whitelist.csv
        
        :param filtered_tokens: List of token data dictionaries that passed all filters
        """
        try:
            os.makedirs(os.path.dirname(WHITELIST_PATH), exist_ok=True)
            
            # Write all token data to CSV
            with open(WHITELIST_PATH, 'w', newline='') as f:
                if not filtered_tokens:
                    # If no tokens passed the filter, create empty file with headers
                    writer = csv.writer(f)
                    writer.writerow(['mint'])
                    return
                
                # Write complete token data
                writer = csv.DictWriter(f, fieldnames=filtered_tokens[0].keys())
                writer.writeheader()
                writer.writerows(filtered_tokens)
                
            self.logger.info(f"Saved {len(filtered_tokens)} tokens to whitelist")
        except Exception as e:
            self.logger.error(f"Error saving to whitelist: {e}")

    def get_filtered_tokens(self) -> List[dict]:
        """
        Returns the list of tokens that passed all filters.
        """
        try:
            df_filtered_tokens = asyncio.run(self.scan_tokens())
            
            if isinstance(df_filtered_tokens, pd.DataFrame):
                if df_filtered_tokens.empty:
                    self.logger.info("No tokens passed the filters")
                    return []
                
                filtered_tokens = df_filtered_tokens.to_dict('records')
                self.logger.info(f"Retrieved {len(filtered_tokens)} filtered tokens")
                return filtered_tokens
            
        except RuntimeError as e:
            if "cannot be called from a running event loop" in str(e):
                # If we're already in an event loop, use it
                loop = asyncio.get_running_loop()
                df_filtered_tokens = loop.run_until_complete(self.scan_tokens())
                
                if isinstance(df_filtered_tokens, pd.DataFrame) and not df_filtered_tokens.empty:
                    filtered_tokens = df_filtered_tokens.to_dict('records')
                    self.logger.info(f"Retrieved {len(filtered_tokens)} filtered tokens")
                    return filtered_tokens
                
        except Exception as e:
            self.logger.error(f"Error getting filtered tokens: {e}")
            return []

    def _apply_additional_filters(self, df):
        """
        Apply all filter modules in the filters directory.
        
        Args:
            df: DataFrame of tokens
            
        Returns:
            Updated DataFrame with filter results
        """
        if not os.path.exists(FILTERS_DIRECTORY):
            logging.warning(f"Filters directory not found: {FILTERS_DIRECTORY}")
            return df
            
        # Get all filter modules
        filter_modules = [f for f in os.listdir(FILTERS_DIRECTORY) 
                         if f.endswith('.py') and not f.startswith('__')]
        
        if not filter_modules:
            logging.info("No filter modules found")
            return df
            
        logging.info(f"Applying {len(filter_modules)} additional filters")
        
        # Apply each filter
        for filter_file in filter_modules:
            module_name = filter_file[:-3]  # Remove .py extension
            filter_col = f"filter_{module_name}"
            
            try:
                # Import the filter module
                filter_module = importlib.import_module(f"filters.{module_name}")
                
                # Apply filter to each token
                for idx, row in df.iterrows():
                    try:
                        token_address = row['address']
                        # Apply the filter
                        result = filter_module.apply_filter(token_address, row.to_dict())
                        # Store result in DataFrame
                        df.at[idx, filter_col] = result
                    except Exception as e:
                        logging.error(f"Error applying filter {module_name} to token {row.get('address', 'unknown')}: {e}")
                        df.at[idx, filter_col] = False
                
                logging.info(f"Applied filter: {module_name}")
            except Exception as e:
                logging.error(f"Error loading filter module {module_name}: {e}")
        
        return df
    
    def _save_tokens_to_db(self, df):
        """
        Save tokens to database.
        Uses TokenDatabase to manage connections.
        
        Args:
            df: DataFrame of tokens
        """
        try:
            # Use TokenDatabase instead of DatabaseConnector
            from data.token_database import TokenDatabase
            
            db = TokenDatabase()
            # Let TokenDatabase handle the database connection and storage
            success = db.store_scanner_results(df)
            
            if success:
                self.logger.info(f"Saved {len(df)} tokens to database")
            else:
                self.logger.error("Failed to save tokens to database")
            
        except Exception as e:
            self.logger.error(f"Error saving tokens to database: {e}")
    
    def _save_tokens_to_csv(self, df):
        """
        Save tokens to CSV file.
        
        Args:
            df: DataFrame of tokens
        """
        try:
            # Ensure output directory exists
            os.makedirs(os.path.dirname(WHITELIST_PATH), exist_ok=True)
            
            # Check if file exists to preserve existing data
            if os.path.exists(WHITELIST_PATH):
                try:
                    # Load existing CSV
                    existing_df = pd.read_csv(WHITELIST_PATH)
                    
                    # Get new tokens not in existing CSV
                    existing_addresses = set(existing_df['address'])
                    new_tokens = df[~df['address'].isin(existing_addresses)]
                    
                    # Update existing tokens with new data
                    for idx, row in df.iterrows():
                        address = row['address']
                        if address in existing_addresses:
                            # Update row in existing DataFrame
                            existing_idx = existing_df[existing_df['address'] == address].index
                            for col in df.columns:
                                if col in existing_df.columns:
                                    existing_df.loc[existing_idx, col] = row[col]
                            
                            # Add any new columns from new DataFrame
                            for col in df.columns:
                                if col not in existing_df.columns:
                                    existing_df[col] = None
                                    existing_df.loc[existing_idx, col] = row[col]
                    
                    # Concatenate existing and new tokens
                    combined_df = pd.concat([existing_df, new_tokens], ignore_index=True)
                    combined_df.to_csv(WHITELIST_PATH, index=False)
                    logging.info(f"Updated whitelist with {len(new_tokens)} new tokens at {WHITELIST_PATH}")
                except Exception as e:
                    logging.error(f"Error updating existing whitelist, overwriting: {e}")
                    df.to_csv(WHITELIST_PATH, index=False)
            else:
                # Save new CSV
                df.to_csv(WHITELIST_PATH, index=False)
                logging.info(f"Created new whitelist with {len(df)} tokens at {WHITELIST_PATH}")
        except Exception as e:
            logging.error(f"Error saving tokens to CSV: {e}")

    async def _fetch_potential_tokens(self) -> list[dict]:
        """
        Fetches potential new tokens, e.g., from DexScreener recent pairs.
        Replace with your actual token discovery logic.
        Returns a list of basic token data dicts, including pair address.
        """
        self.logger.info("Fetching potential tokens (using placeholder DexScreener simulation)...")
        # --- Placeholder Simulation ---
        # In a real scenario, you might query DexScreener for new pairs on Solana,
        # or listen to blockchain events for new pools.
        # This placeholder returns data similar to the user's example structure.
        await asyncio.sleep(0.1) # Simulate network delay
        # Example: Fetch recent pairs for a specific DEX or chain if needed
        # Example structure mimicking a simplified DexScreener API response subset
        # In reality, you'd call the API and parse the actual response.
        # We need the pair address to fetch detailed data later.
        simulated_new_pairs = [
             {'pair_address': 'A7MSQobaXrQEQe8cTtUtwDTorfQLJcwfTGckA4cUeBuY', 'base_token_address': 'Ehcp6YJ6FP2TRZvtAdcps6SrLMsoaUfRmde3QFc7Mv6X', 'symbol': 'TOW'},
             {'pair_address': '9Q79gd7mGrqfEMvLoJnSm62AdjTTM77b5kpCmMTiJBkn', 'base_token_address': 'HDmW3UGw2AA75XpRxs6ZiPpagCKd95vh64YrogdApump', 'symbol': 'SNOWBUNNY'},
             {'pair_address': '69hMAUxywdn4AgaX7CFnKNTEj1j55jRfFsShy1ftGQ5v', 'base_token_address': 'F7dDs2rSQdCipxoaorpLE2awQMK1tjyj6u3DsqFRkuT7', 'symbol': 'MEMORIA'},
             {'pair_address': 'FakePairAddrLowLiq', 'base_token_address': 'FakeTokenAddrLowLiq', 'symbol': 'LOWLIQ'}, # For filter testing
             {'pair_address': 'FakePairAddrFail', 'base_token_address': 'FakeTokenAddrFail', 'symbol': 'FAIL'}, # For error testing
        ]
        self.logger.info(f"Fetched {len(simulated_new_pairs)} potential tokens for detailed check.")
        return simulated_new_pairs
        # --- End Placeholder ---

        # --- Real Implementation Example Snippet (using httpx) ---
        # try:
        #     # Example: Get pairs on Solana, sorted by creation time (desc)
        #     # Adjust endpoint/params based on DexScreener docs for "new pairs"
        #     api_endpoint = "https://api.dexscreener.com/latest/dex/tokens/solana/new_pairs" # Fictional endpoint - check docs
        #     response = await self.http_client.get(api_endpoint)
        #     response.raise_for_status() # Raise exception for bad status codes
        #     data = response.json()
        #     potential_tokens = []
        #     if data and 'pairs' in data:
        #         for pair in data['pairs']:
        #              potential_tokens.append({
        #                  'pair_address': pair.get('pairAddress'),
        #                  'base_token_address': pair.get('baseToken', {}).get('address'),
        #                  'symbol': pair.get('baseToken', {}).get('symbol'),
        #              })
        #     logger.info(f"Fetched {len(potential_tokens)} potential tokens from DexScreener.")
        #     return potential_tokens
        # except httpx.RequestError as e:
        #     logger.error(f"Error fetching potential tokens from DexScreener: {e}", exc_info=True)
        #     return []
        # except Exception as e:
        #     logger.error(f"Unexpected error processing potential tokens: {e}", exc_info=True)
        #     return []
        # --- End Real Implementation Snippet ---

    async def _get_detailed_dex_data(self, pair_addresses: list[str]) -> dict[str, dict]:
        """
        Fetches detailed pair data from DexScreener API for multiple pairs.

        Args:
            pair_addresses: A list of pair addresses.

        Returns:
            A dictionary mapping pair_address to its detailed data dict,
            or None if the request fails. Returns {} if no data found for pairs.
        """
        if not pair_addresses:
            return {}

        # DexScreener API typically takes chain/address format
        # Example for Solana: /latest/dex/pairs/solana/addr1,addr2,...
        chain = "solana" # Assuming Solana, make configurable if needed
        addresses_str = ",".join(pair_addresses)
        url = f"https://api.dexscreener.com/latest/dex/pairs/{chain}/{addresses_str}"
        max_retries = 3
        retry_delay = 2 # seconds

        for attempt in range(max_retries):
            try:
                response = await self.http_client.get(url)
                response.raise_for_status()
                data = response.json()
                if data and 'pairs' in data:
                    # Create a dictionary mapping pairAddress to the pair data object
                    return {pair['pairAddress']: pair for pair in data['pairs']}
                else:
                    self.logger.warning(f"Dexscreener returned no pair data for addresses: {addresses_str}")
                    return {} # Return empty dict if API call succeeded but no pairs found
            except httpx.HTTPStatusError as e:
                 self.logger.error(f"Dexscreener HTTP error fetching detailed data (Status {e.response.status_code}) for {addresses_str}: {e.response.text}")
                 # Don't retry on 404 Not Found, but maybe on server errors (5xx)
                 if e.response.status_code == 404:
                     return {} # Treat as no data found
                 # Consider retrying only on 5xx errors? For now, retry on any HTTP error except 404.
            except httpx.RequestError as e:
                self.logger.error(f"Dexscreener network error fetching detailed data for {addresses_str}: {e}")
            except json.JSONDecodeError as e:
                self.logger.error(f"Dexscreener JSON decode error fetching detailed data for {addresses_str}: {e}")
            except Exception as e:
                self.logger.error(f"Unexpected error fetching detailed data from Dexscreener for {addresses_str}: {e}", exc_info=True)

            # If error occurred and retries remain
            if attempt < max_retries - 1:
                self.logger.warning(f"Retrying Dexscreener fetch in {retry_delay} seconds... (Attempt {attempt + 1}/{max_retries})")
                await asyncio.sleep(retry_delay)
            else:
                self.logger.error(f"Max retries reached fetching detailed data from Dexscreener for {addresses_str}.")
                return None # Indicate failure after retries

        return None # Should not be reached if loop completes, but acts as fallback

    async def _enrich_and_filter_token(self, basic_token_info: dict, detailed_dex_data: dict | None) -> tuple[dict | None, dict | None]:
        """
        Enriches a single token with detailed DexScreener data and applies filters.
        Stores raw DexScreener data in filter_details.

        Args:
            basic_token_info: Dict with at least 'pair_address', 'base_token_address', 'symbol'.
            detailed_dex_data: The raw dictionary for this pair from DexScreener API response, or None if fetch failed.

        Returns:
            (enriched_token_data, filter_details) if passed, else (None, filter_details).
            filter_details always contains processing info, including raw dexscreener data if available.
        """
        pair_address = basic_token_info.get('pair_address')
        token_address = basic_token_info.get('base_token_address') # Use base token address as primary identifier
        symbol = basic_token_info.get('symbol')

        # Initialize filter_details structure
        filter_details = {
            'address': token_address, # Primary key for DB token record
            'pair_address': pair_address,
            'symbol': symbol,
            'scan_timestamp': datetime.now(timezone.utc).isoformat(),
            'dexscreener_fetch_success': False,
            'dexscreener_data': None, # Store raw data here
            'enrichment_complete': False,
            'filters_applied': {}, # Store individual filter results
            'overall_passed': False,
            'error': None
        }

        enriched_data = { # Start building the data to potentially store in DB main columns
            'address': token_address,
            'symbol': symbol,
            'name': None, # Will be filled from Dex data
            'pair_address': pair_address,
            # Add other fields that have dedicated DB columns
            'price': None,
            'liquidity': None,
            'volume24h': None,
            'market_cap': None,
            'status': 'scanned', # Default status after scan
            'last_scanned_at': datetime.now(timezone.utc)
        }

        try:
            # --- Stage 1: Check if detailed DexScreener data is available ---
            if detailed_dex_data is None:
                filter_details['error'] = "Failed to fetch detailed data from DexScreener after retries."
                self.logger.warning(f"Skipping token {symbol} ({token_address}) due to DexScreener fetch failure.")
                return None, filter_details # Cannot proceed without Dex data

            filter_details['dexscreener_fetch_success'] = True
            filter_details['dexscreener_data'] = detailed_dex_data # Store the raw JSON data

            # --- Stage 2: Data Enrichment (Extract key fields from Dex data) ---
            # Use .get() extensively to handle potentially missing fields in API response
            base_token = detailed_dex_data.get('baseToken', {})
            quote_token = detailed_dex_data.get('quoteToken', {})
            txns = detailed_dex_data.get('txns', {})
            m5_txns = txns.get('m5', {})
            h1_txns = txns.get('h1', {})
            h6_txns = txns.get('h6', {})
            h24_txns = txns.get('h24', {})
            volume = detailed_dex_data.get('volume', {})
            price_change = detailed_dex_data.get('priceChange', {})
            liquidity = detailed_dex_data.get('liquidity', {})
            info = detailed_dex_data.get('info', {})
            socials = info.get('socials', [])
            websites = info.get('websites', [])

            enriched_data.update({
                'name': base_token.get('name'),
                'price': float(detailed_dex_data.get('priceUsd', 0)), # Ensure float
                'liquidity': float(liquidity.get('usd', 0)), # Ensure float
                'volume24h': float(volume.get('h24', 0)), # Ensure float
                'market_cap': float(detailed_dex_data.get('marketCap', 0)), # Ensure float
                # Add other fields you want as dedicated columns in the DB
                # These can also be accessed later via filter_details['dexscreener_data']
            })

            # Add extracted details also to filter_details for logging/reference
            filter_details['extracted_dex_fields'] = {
                 'price_native': detailed_dex_data.get('priceNative'),
                 'fdv': detailed_dex_data.get('fdv'),
                 'pair_created_at_ms': detailed_dex_data.get('pairCreatedAt'),
                 'vol_5m': volume.get('m5'), 'vol_1h': volume.get('h1'), 'vol_6h': volume.get('h6'),
                 'price_change_5m': price_change.get('m5'), 'price_change_1h': price_change.get('h1'),
                 'price_change_6h': price_change.get('h6'), 'price_change_24h': price_change.get('h24'),
                 'txns_buys_5m': m5_txns.get('buys'), 'txns_sells_5m': m5_txns.get('sells'),
                 'txns_buys_1h': h1_txns.get('buys'), 'txns_sells_1h': h1_txns.get('sells'),
                 'txns_buys_6h': h6_txns.get('buys'), 'txns_sells_6h': h6_txns.get('sells'),
                 'txns_buys_24h': h24_txns.get('buys'), 'txns_sells_24h': h24_txns.get('sells'),
                 'twitter': next((s['url'] for s in socials if s.get('type') == 'twitter'), None),
                 'telegram': next((s['url'] for s in socials if s.get('type') == 'telegram'), None),
                 'website': next((w['url'] for w in websites if w.get('label') == 'Website'), None),
                 # Add any other fields you frequently check
            }
            filter_details['enrichment_complete'] = True


            # --- Stage 3: Apply Filters based on FiltersConfig ---
            criteria = self.filters_config.criteria
            passed_all_filters = True
            filters_applied = {} # Reset here

            # Example: Liquidity Filter
            min_liq = criteria.get('min_liquidity', 0)
            current_liq = enriched_data.get('liquidity', 0)
            passed_liquidity = current_liq >= min_liq
            filters_applied['liquidity_check'] = {'passed': passed_liquidity, 'value': current_liq, 'threshold': min_liq}
            if not passed_liquidity: passed_all_filters = False

            # Example: Volume Filter
            min_vol = criteria.get('min_volume_24h', 0)
            current_vol = enriched_data.get('volume24h', 0)
            passed_volume = current_vol >= min_vol
            filters_applied['volume_check'] = {'passed': passed_volume, 'value': current_vol, 'threshold': min_vol}
            if not passed_volume: passed_all_filters = False

            # Example: Pair Age Filter
            min_age_minutes = criteria.get('min_pair_age_minutes', 5) # e.g., require pair to be at least 5 mins old
            pair_created_ms = detailed_dex_data.get('pairCreatedAt')
            current_age_minutes = -1 # Default if no creation time
            if pair_created_ms:
                pair_created_ts = pair_created_ms / 1000
                current_ts = datetime.now(timezone.utc).timestamp()
                current_age_minutes = (current_ts - pair_created_ts) / 60
            passed_age = current_age_minutes >= min_age_minutes
            filters_applied['age_check'] = {'passed': passed_age, 'value_minutes': round(current_age_minutes, 2), 'threshold_minutes': min_age_minutes}
            if not passed_age: passed_all_filters = False

            # Example: Transaction Count Filter (e.g., minimum txns in last hour)
            min_txns_1h = criteria.get('min_txns_1h', 10)
            buys_1h = filter_details['extracted_dex_fields'].get('txns_buys_1h', 0) or 0
            sells_1h = filter_details['extracted_dex_fields'].get('txns_sells_1h', 0) or 0
            current_txns_1h = buys_1h + sells_1h
            passed_txns = current_txns_1h >= min_txns_1h
            filters_applied['txn_count_1h_check'] = {'passed': passed_txns, 'value': current_txns_1h, 'threshold': min_txns_1h}
            if not passed_txns: passed_all_filters = False


            # --- Add other filters here (Rug Check, Twitter, Contract Verified etc.) ---
            # These might involve calls to other services or libraries using token_address
            # Example: Placeholder Rug Check
            # rug_score = await self.rug_checker.get_score(token_address) # Needs implementation
            # max_rug = criteria.get('max_rugcheck_score', 50)
            # passed_rug = rug_score <= max_rug
            # filters_applied['rug_check'] = {'passed': passed_rug, 'score': rug_score, 'threshold': max_rug}
            # if not passed_rug: passed_all_filters = False


            # --- Stage 4: Final Decision ---
            filter_details['filters_applied'] = filters_applied
            filter_details['overall_passed'] = passed_all_filters

            if passed_all_filters:
                self.logger.debug(f"Token {symbol} ({token_address}) passed all filters.")
                # Return the structured data for DB main columns, and the full details blob
                return enriched_data, filter_details
            else:
                # Log why it failed using the filters_applied details
                failed_checks = {k: v for k, v in filters_applied.items() if not v['passed']}
                self.logger.debug(f"Token {symbol} ({token_address}) failed filters: {json.dumps(failed_checks)}")
                # Return None for data (won't be whitelisted), but return full filter_details for logging/DB update
                return None, filter_details

        except Exception as e:
            self.logger.error(f"Error processing token {symbol} ({token_address}) in scanner: {e}", exc_info=True)
            filter_details['error'] = str(e)
            filter_details['overall_passed'] = False
            return None, filter_details # Signal failure

    async def scan_and_filter(self) -> list[tuple[dict, dict]]:
        """
        Fetches potential tokens, enriches them using batched DexScreener calls,
        applies filters concurrently, updates DB, and returns passed tokens.
        """
        self.logger.info("Starting token scan and filter process...")
        potential_tokens = await self._fetch_potential_tokens()
        if not potential_tokens:
            self.logger.info("No potential tokens found to process.")
            return []

        # Extract pair addresses for batch fetching
        pair_addresses_to_fetch = [
            p['pair_address'] for p in potential_tokens if p.get('pair_address')
        ]
        if not pair_addresses_to_fetch:
             self.logger.warning("Potential tokens found, but none have pair addresses.")
             return []

        self.logger.info(f"Fetching detailed data for {len(pair_addresses_to_fetch)} pairs...")
        # Fetch detailed data in a batch
        all_detailed_data = await self._get_detailed_dex_data(pair_addresses_to_fetch)

        if all_detailed_data is None:
            self.logger.error("Failed to fetch any detailed data from DexScreener. Aborting scan cycle.")
            # Optionally update DB status for these tokens to indicate fetch failure?
            return []

        self.logger.info(f"Received detailed data for {len(all_detailed_data)} pairs.")

        # Process each token with its detailed data (or lack thereof if fetch failed for specific pair)
        tasks = []
        for basic_info in potential_tokens:
            pair_addr = basic_info.get('pair_address')
            # Get the detailed data for this specific pair from the batch result
            detailed_data_for_token = all_detailed_data.get(pair_addr) if pair_addr else None
            if pair_addr and not detailed_data_for_token and pair_addr in all_detailed_data:
                 # This case means the API call was successful, but this specific pair wasn't in the response
                 self.logger.debug(f"No detailed data returned by DexScreener for pair {pair_addr}, though API call succeeded.")
                 # We still pass None to _enrich_and_filter_token to handle it gracefully

            tasks.append(self._enrich_and_filter_token(basic_info, detailed_data_for_token))

        results = await asyncio.gather(*tasks, return_exceptions=True)

        passed_tokens_with_details = []
        processed_count = 0
        passed_count = 0
        failed_count = 0
        error_count = 0
        update_tasks = [] # Tasks for DB updates

        for i, result in enumerate(results):
            processed_count += 1
            token_address_for_log = potential_tokens[i].get('base_token_address', 'unknown') # Get address for logging context

            if isinstance(result, Exception):
                self.logger.error(f"Task failed during enrichment/filtering for token {token_address_for_log}: {result}", exc_info=result)
                # Log failure to DB even if task crashed
                fail_details = {'address': token_address_for_log, 'error': f'Processing task exception: {result}', 'overall_passed': False, 'scan_timestamp': datetime.now(timezone.utc).isoformat()}
                update_tasks.append(self.db.add_or_update_token({'address': token_address_for_log, 'status': 'scan_error'}, fail_details))
                error_count += 1
                continue
            elif isinstance(result, tuple) and len(result) == 2:
                enriched_data, filter_details = result
                token_address_from_details = filter_details.get('address')

                if not token_address_from_details:
                     self.logger.warning(f"Could not process a scan result: Missing address in filter_details. Details: {filter_details}")
                     error_count += 1
                     continue # Skip DB update if no address

                # --- IMPORTANT: Update DB with scan results (passed or failed) ---
                # Use the enriched data if available (passed filters), otherwise use basic info + status
                data_to_update = enriched_data if enriched_data else {'address': token_address_from_details, 'status': 'scanned_failed_filters'}
                # Ensure essential fields for DB are present if only address is known
                data_to_update.setdefault('symbol', filter_details.get('symbol'))
                data_to_update.setdefault('name', None) # Name might not be known if enrichment failed early
                data_to_update.setdefault('last_scanned_at', datetime.now(timezone.utc))

                # Add the DB update task to the list to run concurrently
                update_tasks.append(self.db.add_or_update_token(data_to_update, filter_details))

                # Only add to the return list if it passed *and* data is present
                if enriched_data and filter_details.get('overall_passed', False):
                    passed_tokens_with_details.append((enriched_data, filter_details))
                    passed_count += 1
                elif filter_details.get('error'):
                    error_count += 1 # Count errors identified within the processing function
                else:
                    failed_count += 1 # Count explicit filter failures
            else:
                self.logger.warning(f"Unexpected result format from _enrich_and_filter_token for {token_address_for_log}: {result}")
                error_count += 1

        # Wait for all DB updates to complete
        if update_tasks:
            self.logger.info(f"Waiting for {len(update_tasks)} DB updates from scanner...")
            db_update_results = await asyncio.gather(*update_tasks, return_exceptions=True)
            # Log any errors during DB updates
            for i, db_res in enumerate(db_update_results):
                if isinstance(db_res, Exception) or db_res is False:
                    # Attempt to find corresponding token address for logging context
                    # This is tricky as tasks might finish out of order. Best effort:
                    self.logger.error(f"Failed DB update from scanner task result {i}: {db_res}", exc_info=isinstance(db_res, Exception))


        self.logger.info(f"Scan and filter process completed. Processed: {processed_count}, Passed Filters: {passed_count}, Failed Filters: {failed_count}, Errors: {error_count}")
        return passed_tokens_with_details

    async def close(self):
        """Closes the HTTP client."""
        await self.http_client.aclose()
        self.logger.info("TokenScanner HTTP client closed.")

    async def apply_rug_pull_filter(self, tokens: List[Dict]) -> List[Dict]:
        """
        Apply additional rug pull checks using RugPullFilter.
        
        Args:
            tokens: List of token data
            
        Returns:
            List of filtered tokens
        """
        from filters.rug_filter import RugPullFilter
        
        rug_filter = RugPullFilter(
            rug_pull_score_threshold=self.thresholds.RUG_PULL_SCORE_THRESHOLD,
            dev_wallet_activity_threshold=self.thresholds.DEV_WALLET_ACTIVITY_THRESHOLD
        )
        
        filtered_tokens = []
        for token in tokens:
            try:
                analysis = rug_filter.analyze_token(token)
                if not analysis["flagged"]:
                    filtered_tokens.append(token)
            except Exception as e:
                self.logger.error(f"Error applying rug pull filter to {token.get('mint')}: {e}")
                
        return filtered_tokens

    async def apply_additional_filters(self, tokens: List[Dict]) -> List[Dict]:
        """
        Apply all additional filters from the filters directory.
        
        Args:
            tokens: List of token data
            
        Returns:
            List of filtered tokens
        """
        filtered_tokens = tokens
        
        # Import and apply each filter
        filter_modules = [
            "blacklist",
            "whitelist",
            "volume_filter",
            "liquidity_filter",
            "whale_filter",
            "scam_filter"
        ]
        
        for module_name in filter_modules:
            try:
                module = importlib.import_module(f"filters.{module_name}")
                if hasattr(module, "filter_tokens"):
                    filtered_tokens = await module.filter_tokens(filtered_tokens)
            except Exception as e:
                self.logger.error(f"Error applying {module_name} filter: {e}")
                
        return filtered_tokens

    async def process_tokens(self) -> None:
        """Main token processing flow with improved error handling."""
        try:
            # 1. Scan for new tokens
            self.logger.info("Starting token scan...")
            tokens = await self._fetch_potential_tokens()
            
            if not tokens:
                self.logger.warning("No new tokens found to process")
                return
            
            self.logger.info(f"Found {len(tokens)} potential tokens to process")
            
            # 2. Apply filters and store results
            self.logger.info("Applying filters...")
            processed_count = 0
            success_count = 0
            
            for token in tokens:
                try:
                    processed_count += 1
                    token_mint = token.get('mint')
                    
                    if not token_mint:
                        self.logger.warning(f"Skipping token without mint address: {token}")
                        continue
                    
                    # Apply rug check filter
                    self.logger.info(f"Applying rug check filter to {token_mint}")
                    rug_result = await self.apply_rug_pull_filter([token])
                    rug_passed = len(rug_result) > 0
                    
                    await self.db.store_filter_result(
                        token_mint,
                        'rug_check',
                        rug_passed,
                        details=json.dumps({
                            'risk_score': token.get('rugcheckScore'),
                            'timestamp': datetime.now(timezone.utc).isoformat()
                        })
                    )
                    
                    if not rug_passed:
                        self.logger.info(f"Token {token_mint} failed rug check filter")
                        continue
                    
                    # Apply additional filters
                    self.logger.info(f"Applying additional filters to {token_mint}")
                    additional_filters = await self.apply_additional_filters([token])
                    filters_passed = len(additional_filters) > 0
                    
                    await self.db.store_filter_result(
                        token_mint,
                        'additional_filters',
                        filters_passed,
                        details=json.dumps({
                            'filter_results': additional_filters,
                            'timestamp': datetime.now(timezone.utc).isoformat()
                        })
                    )
                    
                    if not filters_passed:
                        self.logger.info(f"Token {token_mint} failed additional filters")
                        continue
                    
                    # Store token data
                    await self.db.add_or_update_token(token)
                    success_count += 1
                    self.logger.info(f"Successfully processed token {token_mint}")
                    
                except Exception as e:
                    self.logger.error(f"Error processing token {token_mint}: {e}", exc_info=True)
                    continue
                
            self.logger.info(f"Token processing completed. Processed: {processed_count}, Success: {success_count}")
            
            # 3. Get filtered tokens for strategy selection
            self.logger.info("Retrieving filtered tokens for strategy selection...")
            filtered_tokens = await self.db.get_filtered_tokens('rug_check', True)
            filtered_tokens = await self.db.get_filtered_tokens('additional_filters', True)
            
            if not filtered_tokens:
                self.logger.warning("No tokens passed all filters")
                return
            
            self.logger.info(f"Found {len(filtered_tokens)} tokens that passed all filters")
            
            # 4. Process each filtered token
            for token_mint in filtered_tokens:
                try:
                    token_data = await self.db.get_token_data(token_mint)
                    if not token_data:
                        self.logger.warning(f"No data found for filtered token {token_mint}")
                        continue
                    
                    # Get platform data
                    self.logger.info(f"Getting platform data for {token_mint}")
                    platform_data = await self.platform_tracker.get_platform_data(token_mint)
                    
                    # Get token metrics
                    self.logger.info(f"Getting metrics for {token_mint}")
                    metrics = await self.db.get_token_deltas(token_mint)
                    
                    # Combine data for strategy selection
                    token_info = {
                        **token_data,
                        'platform_data': platform_data,
                        'metrics': metrics,
                        'last_updated': datetime.now(timezone.utc).isoformat()
                    }
                    
                    # Store in database for strategy selection
                    await self.db.add_or_update_token(token_info)
                    self.logger.info(f"Successfully updated token info for {token_mint}")
                    
                except Exception as e:
                    self.logger.error(f"Error processing filtered token {token_mint}: {e}", exc_info=True)
                    continue
                
            self.logger.info("Token processing completed successfully")
            
        except Exception as e:
            self.logger.error(f"Critical error in token processing: {e}", exc_info=True)
            raise

    async def _get_rugcheck_score(self, token_mint: str) -> Optional[float]:
        """Get rugcheck score with improved rate limiting and retries."""
        try:
            # Add longer initial delay between requests
            await asyncio.sleep(2)  # 2 second delay between requests
            
            # Get API key from settings
            rugcheck_api_key = self.settings.get('RUGCHECK_API_KEY')
            if not rugcheck_api_key:
                self.logger.warning("RugCheck API key not found in settings")
                return None
            
            # Configure headers with API key
            headers = {
                'Authorization': f'Bearer {rugcheck_api_key}',
                'Accept': 'application/json'
            }
            
            # Add more sophisticated retry logic
            max_retries = 5  # Increased from 3
            base_delay = 3  # Base delay in seconds
            max_delay = 30  # Maximum delay in seconds
            
            for attempt in range(max_retries):
                try:
                    # Get proxy for this request
                    proxy = self.proxy_manager.get_proxy_dict()
                    
                    async with aiohttp.ClientSession() as session:
                        async with session.get(
                            f"https://api.rugcheck.xyz/v1/tokens/{token_mint}/report/summary",
                            headers=headers,
                            proxy=proxy['https'] if proxy else None,
                            timeout=30  # Add timeout
                        ) as response:
                            if response.status == 429:  # Rate limit hit
                                if proxy:
                                    self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                                if attempt < max_retries - 1:
                                    # Exponential backoff with jitter
                                    wait_time = min(
                                        base_delay * (2 ** attempt) + random.uniform(0, 2),
                                        max_delay
                                    )
                                    self.logger.warning(
                                        f"Rate limit hit for {token_mint}, "
                                        f"attempt {attempt + 1}/{max_retries}, "
                                        f"waiting {wait_time:.1f} seconds"
                                    )
                                    await asyncio.sleep(wait_time)
                                    continue
                                else:
                                    self.logger.error(f"Max retries reached for RugCheck API for {token_mint}")
                                    return None
                            
                            elif response.status == 504:  # Gateway Timeout
                                if proxy:
                                    self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                                if attempt < max_retries - 1:
                                    wait_time = base_delay * (attempt + 1)
                                    self.logger.warning(
                                        f"Gateway timeout for {token_mint}, "
                                        f"attempt {attempt + 1}/{max_retries}, "
                                        f"waiting {wait_time} seconds"
                                    )
                                    await asyncio.sleep(wait_time)
                                    continue
                                else:
                                    self.logger.error(f"Max retries reached due to gateway timeouts for {token_mint}")
                                    return None
                            
                            response.raise_for_status()
                            data = await response.json()
                            score = float(data.get('score', 0))
                            self.logger.info(f"Successfully got RugCheck score {score} for {token_mint}")
                            return score
                        
                except asyncio.TimeoutError:
                    if proxy:
                        self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                    self.logger.warning(f"Timeout on attempt {attempt + 1} for {token_mint}")
                    if attempt < max_retries - 1:
                        await asyncio.sleep(base_delay * (attempt + 1))
                        continue
                    else:
                        self.logger.error(f"Max retries reached due to timeouts for {token_mint}")
                        return None
                    
                except aiohttp.ClientError as e:
                    if proxy:
                        self.proxy_manager.mark_proxy_failure(self.proxy_manager.current_proxy)
                    if attempt < max_retries - 1:
                        self.logger.warning(
                            f"Request failed for {token_mint}, "
                            f"attempt {attempt + 1}/{max_retries}: {e}"
                        )
                        await asyncio.sleep(base_delay * (attempt + 1))
                        continue
                    else:
                        self.logger.error(f"Failed to get RugCheck score after {max_retries} attempts for {token_mint}: {e}")
                        return None
                    
        except Exception as e:
            self.logger.error(f"Unexpected error in RugCheck score calculation for {token_mint}: {e}", exc_info=True)
            return None